{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPEYaS0dA_Pd"
      },
      "source": [
        "# Studying a model performance with synthetic data\n",
        "\n",
        "Synthetic data have been created for each class, using as input the pictures that all models misclassify. The idea is the following: Maybe having more pics similar to the misclassified ones we can improve the performance of the model in that areas.\n",
        "\n",
        "- There are a test set with 150 real pics of each class.\n",
        "- Now, for each class, there are other 3 train/validation folders:\n",
        "  - With all real pics and 50 synthetic pics\n",
        "  - With all real pics and 250 synthetic pics\n",
        "  - With all real pics and 480 synthetic pics\n",
        "\n",
        "\n",
        "- The study of what pics were generated synthetically can be found here => https://github.com/albertovpd/viu_tfm-deep_vision_classification/tree/synthetic_data_study\n",
        "\n",
        "- They were created using the pytorch implementation of this repo => https://github.com/mit-han-lab/data-efficient-gans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnrItHohA1UI"
      },
      "outputs": [],
      "source": [
        "# Google Drive stuff\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXg1WfU_B6SI"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I_tE0o9CCGh"
      },
      "outputs": [],
      "source": [
        "# tf\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ-gURRuCOla"
      },
      "source": [
        "- libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpShKkBeCL48"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "# batch ingestion of pics without pickle\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# nns\n",
        "from tensorflow.keras.applications import ResNet50 \n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model # Sequential\n",
        "from tensorflow.keras import layers \n",
        "\n",
        "# optimization\n",
        "from tensorflow.keras.optimizers import SGD #Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# nn architectures, metrics, viz & reports => written in my_functions202202 file\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/src\")\n",
        "from my_functions202202 import generic_last_2layers, plotting_model, model_evaluation, classification_report_pic, confusion_matrix_report\n",
        "\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "# navigating through folder\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9du-AOzCVQ5"
      },
      "source": [
        "- paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU4Q7459CTE1"
      },
      "outputs": [],
      "source": [
        "base_folder = \"/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/\"\n",
        "trainval_folders = base_folder + \"input/dataset_synth_data-1test_3trainval/train_val_ds/\"\n",
        "test_folder = base_folder+\"input/dataset_synth_data-1test_3trainval/test_ds/\"\n",
        "\n",
        "src_folder =  base_folder+\"src/\"\n",
        "output_folder = base_folder + \"/output/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENhx47E8C9ul"
      },
      "source": [
        "- functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSaIpHb7C98L"
      },
      "outputs": [],
      "source": [
        "#my_functions202202.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdQj8JY5DV-p"
      },
      "source": [
        "- common parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLqarfuvDvYi"
      },
      "outputs": [],
      "source": [
        "image_size = (128,128)\n",
        "batch_size = 128\n",
        "epochs = 250\n",
        "opt = SGD(momentum=0.9) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq54z9khFNLI"
      },
      "outputs": [],
      "source": [
        "base_model_resnet50 = ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3), classes = 5) \n",
        "models_dict = {\"resnet50_NOdataAug_dropoutFirst007\": \n",
        "         generic_last_2layers(data_augmentation= None, nn=base_model_resnet50, neurons_final_layer=5, dropout_layers=True,  dropout_position=\"first\",  dropout_percent = 0.07)\n",
        "         }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrnMLMpIja6S"
      },
      "outputs": [],
      "source": [
        "test_ds = image_dataset_from_directory(\n",
        "    test_folder,\n",
        "      class_names=[\"Bedroom\",\"Bathroom\",\"Dinning\",\"Livingroom\",\"Kitchen\"],\n",
        "      seed=None,\n",
        "      validation_split=None, \n",
        "      subset=None,\n",
        "      image_size= image_size,\n",
        "      batch_size= batch_size,\n",
        "      color_mode='rgb',\n",
        "      shuffle=False \n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T1neSKnEVxx"
      },
      "source": [
        "# Irregular partitions for train set\n",
        "\n",
        "The motivation is to check performance vs lack of data. Then:\n",
        "\n",
        "- 150 pics for each class are saved in the val_ds (indeed, the very same than before)\n",
        "- with the rest of them, 4 subfolders are created. each time the data is randomly shuffled:\n",
        "  - 20% are stored in test_ds folder\n",
        "  - 80%, 62%, 46%, 30% of the remaining pics are saved for the train dataset. The rest is discarded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4jpVlphExtP"
      },
      "outputs": [],
      "source": [
        "os.listdir(trainval_folders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq2OI7zGEx5j"
      },
      "outputs": [],
      "source": [
        "# total = 0\n",
        "# for root, dirs, files in os.walk(trainval_folders):\n",
        "#     total += len(files)\n",
        "#     print(dirs, len(files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmYDXoL0UfZT"
      },
      "outputs": [],
      "source": [
        "# remaining_folders_irr = os.listdir(irreg_input)[2:3]\n",
        "folders = os.listdir(trainval_folders)\n",
        "folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds1JAkI6Ex89"
      },
      "outputs": [],
      "source": [
        "for f in folders:\n",
        "    print(\"\\n ================================\",\n",
        "          \"\\n FOLDER : \",f)\n",
        "    print(\"train dataset\")\n",
        "    train_path = trainval_folders+f+\"/\"+'train_ds/'\n",
        "    print(train_path)\n",
        "    train_ds = image_dataset_from_directory(\n",
        "        train_path,\n",
        "        class_names=[\"Bedroom\",\"Bathroom\",\"Dinning\",\"Livingroom\",\"Kitchen\"],\n",
        "        seed=None,\n",
        "        validation_split=None, \n",
        "        subset=None,\n",
        "        image_size= image_size,\n",
        "        batch_size= batch_size,\n",
        "        color_mode='rgb',\n",
        "        shuffle=False \n",
        "        )\n",
        "\n",
        "    print(\"\\n val dataset\")\n",
        "    val_path = trainval_folders+f+\"/\"+\"val_ds\"\n",
        "    val_ds = image_dataset_from_directory(\n",
        "      val_path,\n",
        "      class_names=[\"Bedroom\",\"Bathroom\",\"Dinning\",\"Livingroom\",\"Kitchen\"],\n",
        "      seed=None,\n",
        "      validation_split=None, \n",
        "      subset=None,\n",
        "      image_size= image_size,\n",
        "      batch_size= batch_size,\n",
        "      color_mode='rgb',\n",
        "      shuffle=False \n",
        "    )\n",
        "\n",
        "    class_names = train_ds.class_names\n",
        "    print(class_names)\n",
        "\n",
        "    # checking numbers non sense now because all have same volume\n",
        "    paths = ['train_ds/', 'val_ds/']\n",
        "    for p in paths:\n",
        "        for dir,subdir,files in os.walk(trainval_folders+f+\"/\"+p):\n",
        "            print(dir,' => ', p, str(len(files)))\n",
        "\n",
        "    # calling model\n",
        "    model_name, nn = list(models_dict.items())[0]\n",
        "    print(\"\\n\", model_name)\n",
        "    nn.summary()\n",
        "\n",
        "    nn.compile( optimizer = opt, #\"adam\", \n",
        "                  loss=SparseCategoricalCrossentropy(from_logits=True) ,#'categorical_crossentropy', \n",
        "                  metrics=['accuracy'] # \"recall\"\n",
        "                  )\n",
        "\n",
        "    # ====== USING VAL DATASET ======\n",
        "\n",
        "    history = nn.fit(\n",
        "          train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=epochs,\n",
        "          #callbacks = callbacks # <=== REMOVE CALLBACK for full results\n",
        "          )\n",
        "\n",
        "    number_of_epochs_it_ran = len(history.history['loss']) \n",
        "    print(\"run epochs: \",number_of_epochs_it_ran)\n",
        "    name = model_name+\"_irregKfolds_\"+f\n",
        "    #models_dict[m].save(output_folder+name+\".h5\")\n",
        "\n",
        "    # saving model accuracy/loss graph\n",
        "    plotting_model(history,number_of_epochs_it_ran, name, output_folder, \"val\") \n",
        "\n",
        "    # saving model metrics to json\n",
        "    evaluation = nn.evaluate(test_ds, batch_size=batch_size, return_dict=True)\n",
        "    model_evaluation(evaluation, output_folder, name+\"_trainVal\")\n",
        "\n",
        "    # get inferences\n",
        "    y_pred_val_float = nn.predict(val_ds)\n",
        "    y_pred_val = np.argmax(y_pred_val_float, axis=1)\n",
        "\n",
        "    # get real labels\n",
        "    y_target = tf.concat([y for x, y in val_ds], axis=0) \n",
        "\n",
        "    # classification and confusion matrix reports\n",
        "    classification_report_pic(y_pred_val, y_target,  class_names, output_folder, name+\"_trainVal\")\n",
        "    confusion_matrix_report(y_pred_val, y_target, class_names, output_folder, name+\"_trainVal\")\n",
        "\n",
        "    # ====== USING TEST DATASET ======\n",
        "\n",
        "    # saving model metrics to json\n",
        "    evaluation_test = nn.evaluate(test_ds, batch_size=batch_size, return_dict=True)\n",
        "    model_evaluation(evaluation_test, output_folder, name+\"_trainTest\")\n",
        "\n",
        "    # get inferences\n",
        "    y_pred_test_float = nn.predict(test_ds)\n",
        "    y_pred_test = np.argmax(y_pred_test_float, axis=1)\n",
        "\n",
        "    # get real labels for val_ds\n",
        "    y_target_test = tf.concat([y for x, y in test_ds], axis=0) \n",
        "\n",
        "    # classification and confusion matrix reports\n",
        "    classification_report_pic(y_pred_test, y_target_test,  class_names, output_folder, name+\"_trainTest\")\n",
        "    confusion_matrix_report(y_pred_test, y_target_test, class_names, output_folder, name+\"_trainTest\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n_wCtMxyt3Dj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "synth-1test_3trainval_folders.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}