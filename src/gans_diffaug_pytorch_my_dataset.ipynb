{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXt0ZRb8Fpcg"
      },
      "source": [
        "https://colab.research.google.com/gist/zsyzzsoft/5fbb71b9bf9a3217576bebae5de46fc2/data-efficient-gans.ipynb\n",
        "\n",
        "Thanks to:\n",
        "- Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song\n",
        "- Differentiable Augmentation for Data-Efficient GAN Training\n",
        "- Conference on Neural Information Processing Systems (NeurIPS)\n",
        "- 2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALviHR0CH0bq",
        "outputId": "0fcb87a0-8fd3-4ed9-a1e5-34ac8245c7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Google Drive stuff\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRu5xhYI25KW"
      },
      "outputs": [],
      "source": [
        "#pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l74gDsK06u1u"
      },
      "outputs": [],
      "source": [
        "#!apt install -y -q ninja-build cuda-libraries-dev-11.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdLk7kpzoHj4",
        "outputId": "59c0d13d-2490-43fa-eb4a-f94e0657127b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B4GJBAwU5Y-h",
        "outputId": "c4b64cb1-b6dd-4478-a50e-47049fe85652"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sr82R4t0H0eb"
      },
      "outputs": [],
      "source": [
        "# navigating through folder\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UHNG8_ReH0hP"
      },
      "outputs": [],
      "source": [
        "# base_folder = \"/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/\" #input/common_misclassifications/\"\n",
        "# src_folder =  base_folder+\"src/\"\n",
        "# output_folder = base_folder + \"/output/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Uc5NkzDFloM"
      },
      "outputs": [],
      "source": [
        "# !ls \"/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjNwCVkNI8Le"
      },
      "source": [
        "- 1st test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDGpy_ZwSBGy",
        "outputId": "3bf3042d-fda2-4644-fafd-3fea9fe22939"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIdG1AThSHiy",
        "outputId": "d6ce1603-cded-4762-b665-ab2ff0c1d0b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bath_collage.jpg  Bathroom\t  bathroom.zip\tdata-efficient-gans  Kitchen\n",
            "bath_collage.zip  Bathroom_synth  Bedroom\tDinning\t\t     Livingroom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MW5SKcoHDvw",
        "outputId": "8646854b-2bfb-4eb2-8a13-fc36dd1b054a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'data-efficient-gans' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mit-han-lab/data-efficient-gans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6st3FQbScZb",
        "outputId": "f4e05533-9165-43af-f579-44b56c5d6a9c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bath_collage.jpg  Bathroom\t  bathroom.zip\tdata-efficient-gans  Kitchen\n",
            "bath_collage.zip  Bathroom_synth  Bedroom\tDinning\t\t     Livingroom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WpUbbBRKHaw",
        "outputId": "bb623f7f-ce34-4431-9d6b-96567c4553f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/data-efficient-gans/DiffAugment-stylegan2-pytorch\n"
          ]
        }
      ],
      "source": [
        "%cd data-efficient-gans/DiffAugment-stylegan2-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdE0bX3qLvVn",
        "outputId": "eec39537-660e-47cf-b9d7-d06052593f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calc_metrics.py         docker_run.sh    \u001b[0m\u001b[01;34mmetrics\u001b[0m/         \u001b[01;34mtorch_utils\u001b[0m/\n",
            "dataset_tool.py         generate_gif.py  projector.py     \u001b[01;34mtraining\u001b[0m/\n",
            "DiffAugment_pytorch.py  generate.py      \u001b[01;34m__pycache__\u001b[0m/     train.py\n",
            "\u001b[01;34mdnnlib\u001b[0m/                 legacy.py        README.md\n",
            "Dockerfile              LICENSE.txt      style_mixing.py\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m6hOBhI2TjW",
        "outputId": "317d2ccc-b69c-4f13-a561-22a40ab5e859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: train.py [OPTIONS]\n",
            "\n",
            "  Train a GAN using the techniques described in\n",
            "  the paper \"Training Generative Adversarial\n",
            "  Networks with Limited Data\".\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  # Train with custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\n",
            "      --gpus=2 --cfg=cifar --cond=1\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\n",
            "      --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\n",
            "      --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\n",
            "\n",
            "  Base configs (--cfg):\n",
            "    auto       Automatically select reasonable defaults based on resolution\n",
            "               and GPU count. Good starting point for new datasets.\n",
            "    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "    paper1024  Reproduce results for MetFaces at 1024x1024.\n",
            "    cifar      Reproduce results for CIFAR-10 at 32x32.\n",
            "\n",
            "  Transfer learning source networks (--resume):\n",
            "    ffhq256        FFHQ trained at 256x256 resolution.\n",
            "    ffhq512        FFHQ trained at 512x512 resolution.\n",
            "    ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "    celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "    lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "    <PATH or URL>  Custom network pickle.\n",
            "\n",
            "Options:\n",
            "  --outdir DIR                    Where to save\n",
            "                                  the results\n",
            "                                  [required]\n",
            "\n",
            "  --gpus INT                      Number of GPUs\n",
            "                                  to use [default:\n",
            "                                  1]\n",
            "\n",
            "  --snap INT                      Snapshot\n",
            "                                  interval\n",
            "                                  [default: 50\n",
            "                                  ticks]\n",
            "\n",
            "  --metrics LIST                  Comma-separated\n",
            "                                  list or \"none\"\n",
            "                                  [default:\n",
            "                                  fid50k_full]\n",
            "\n",
            "  --seed INT                      Random seed\n",
            "                                  [default: 0]\n",
            "\n",
            "  -n, --dry-run                   Print training\n",
            "                                  options and exit\n",
            "\n",
            "  --data PATH                     Training data\n",
            "                                  (directory or\n",
            "                                  zip)  [required]\n",
            "\n",
            "  --cond BOOL                     Train\n",
            "                                  conditional\n",
            "                                  model based on\n",
            "                                  dataset labels\n",
            "                                  [default: false]\n",
            "\n",
            "  --subset INT                    Train with only\n",
            "                                  N images\n",
            "                                  [default: all]\n",
            "\n",
            "  --mirror BOOL                   Enable dataset\n",
            "                                  x-flips\n",
            "                                  [default: false]\n",
            "\n",
            "  --cfg [low_shot|auto|stylegan2|paper256|paper512|paper1024|cifar]\n",
            "                                  Base config\n",
            "                                  [default:\n",
            "                                  low_shot]\n",
            "\n",
            "  --gamma FLOAT                   Override R1\n",
            "                                  gamma\n",
            "\n",
            "  --kimg INT                      Override\n",
            "                                  training\n",
            "                                  duration\n",
            "\n",
            "  --batch INT                     Override batch\n",
            "                                  size\n",
            "\n",
            "  --DiffAugment TEXT              Comma-separated\n",
            "                                  list of\n",
            "                                  DiffAugment\n",
            "                                  policy [default:\n",
            "                                  color,translatio\n",
            "                                  n,cutout]\n",
            "\n",
            "  --aug [noaug|ada|fixed]         Augmentation\n",
            "                                  mode [default:\n",
            "                                  ada]\n",
            "\n",
            "  --p FLOAT                       Augmentation\n",
            "                                  probability for\n",
            "                                  --aug=fixed\n",
            "\n",
            "  --target FLOAT                  ADA target value\n",
            "                                  for --aug=ada\n",
            "\n",
            "  --augpipe [blit|geom|color|filter|noise|cutout|bg|bgc|bgcf|bgcfn|bgcfnc]\n",
            "                                  Augmentation\n",
            "                                  pipeline\n",
            "                                  [default: bgc]\n",
            "\n",
            "  --resume PKL                    Resume training\n",
            "                                  [default:\n",
            "                                  noresume]\n",
            "\n",
            "  --freezed INT                   Freeze-D\n",
            "                                  [default: 0\n",
            "                                  layers]\n",
            "\n",
            "  --fp32 BOOL                     Disable mixed-\n",
            "                                  precision\n",
            "                                  training\n",
            "\n",
            "  --nhwc BOOL                     Use NHWC memory\n",
            "                                  format with FP16\n",
            "\n",
            "  --nobench BOOL                  Disable cuDNN\n",
            "                                  benchmarking\n",
            "\n",
            "  --allow-tf32 BOOL               Allow PyTorch to\n",
            "                                  use TF32\n",
            "                                  internally\n",
            "\n",
            "  --workers INT                   Override number\n",
            "                                  of DataLoader\n",
            "                                  workers\n",
            "\n",
            "  --help                          Show this\n",
            "                                  message and\n",
            "                                  exit.\n"
          ]
        }
      ],
      "source": [
        "!python train.py --help"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --outdir=\"/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/Bathroom_synth/\" --data=\"/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/bath_collage.zip\" --gpus=1 --workers=1 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS2-2Y7QTZ-m",
        "outputId": "bd32d218-1f25-4572-9cc8-2cd37a63511f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/bath_collage.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 1,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 2000\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 1,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 32768,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 10,\n",
            "    \"diffaugment\": \"color,translation,cutout\"\n",
            "  },\n",
            "  \"total_kimg\": 300,\n",
            "  \"batch_size\": 8,\n",
            "  \"batch_gpu\": 8,\n",
            "  \"ema_kimg\": 10,\n",
            "  \"ema_rampup\": null,\n",
            "  \"run_dir\": \"/content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/Bathroom_synth/00000-bath_collage-low_shot-color-translation-cutout\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/Bathroom_synth/00000-bath_collage-low_shot-color-translation-cutout\n",
            "Training data:      /content/drive/My Drive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/bath_collage.zip\n",
            "Training duration:  300 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   1\n",
            "Image resolution:   2000\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  1\n",
            "Image shape: [3, 2000, 2000]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 554, in <module>\n",
            "    main() # pylint: disable=no-value-for-parameter\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/decorators.py\", line 21, in new_func\n",
            "    return f(get_current_context(), *args, **kwargs)\n",
            "  File \"train.py\", line 547, in main\n",
            "    subprocess_fn(rank=0, args=args, temp_dir=temp_dir)\n",
            "  File \"train.py\", line 398, in subprocess_fn\n",
            "    training_loop.training_loop(rank=rank, **args)\n",
            "  File \"/content/drive/MyDrive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/data-efficient-gans/DiffAugment-stylegan2-pytorch/training/training_loop.py\", line 150, in training_loop\n",
            "    G = dnnlib.util.construct_class_by_name(**G_kwargs, **common_kwargs).train().requires_grad_(False).to(device) # subclass of torch.nn.Module\n",
            "  File \"/content/drive/MyDrive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/data-efficient-gans/DiffAugment-stylegan2-pytorch/dnnlib/util.py\", line 289, in construct_class_by_name\n",
            "    return call_func_by_name(*args, func_name=class_name, **kwargs)\n",
            "  File \"/content/drive/MyDrive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/data-efficient-gans/DiffAugment-stylegan2-pytorch/dnnlib/util.py\", line 284, in call_func_by_name\n",
            "    return func_obj(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/data-efficient-gans/DiffAugment-stylegan2-pytorch/torch_utils/persistence.py\", line 104, in __init__\n",
            "    super().__init__(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/data-efficient-gans/DiffAugment-stylegan2-pytorch/training/networks.py\", line 493, in __init__\n",
            "    self.synthesis = SynthesisNetwork(w_dim=w_dim, img_resolution=img_resolution, img_channels=img_channels, **synthesis_kwargs)\n",
            "  File \"/content/drive/MyDrive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/data-efficient-gans/DiffAugment-stylegan2-pytorch/torch_utils/persistence.py\", line 104, in __init__\n",
            "    super().__init__(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/2-Estudios/viu-master_ai/tfm-deep_vision/input/common_misclassifications/data-efficient-gans/DiffAugment-stylegan2-pytorch/training/networks.py\", line 434, in __init__\n",
            "    assert img_resolution >= 4 and img_resolution & (img_resolution - 1) == 0\n",
            "AssertionError\n",
            "THCudaCheck FAIL file=../aten/src/THC/THCCachingHostAllocator.cpp line=280 error=4 : driver shutting down\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FpgL3WM9wLBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "gans_diffaug-pytorch-my_dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}